---
title: "自然言語処理の基礎感想"
date: 2023-09-21
categories: ["本まとめ"]
tags: ["自然言語処理の基礎"]
---

ChatGPTないしLLMを活用したサービス研究をする中、自然言語処理について基礎から学ぶ必要性に駆られ、自然言語処理を研究していた方にこの本を勧められて購入しました。

正直なところ、機械学習の初学者にとってこの本は難しかったです。それでも自然言語処理について基礎的な部分からどのように発展してきたか、そしてGPTまで体系的に流れを学べたことには価値がありました。トピックごとに本からキーワードをピックアップし、ChatGPTに質問して前提知識を増やしながら読むことでかなり勉強になりました。以下のまとめでは、本に書いていることだけではなくChatGPTやネットの記事を参照して補完し、この本を読む前の自分が理解できるレベルで書いています。誤りや適切ではない表現を使っている点があるかもしれませんが、ご了承ください。

1章は自然言語処理の基本的な部分の解説です。自然言語を処理する上での大きな課題である「曖昧性」、1つの言葉が複数の解釈が可能な点を説明しています。基本的な自然言語へのアプローチとして「形態素解析」「構文解析」「意味解析」と呼ばれる、文章を単語などの単位に分割し、文法的、構造的に解釈する方法論を紹介しています。

2章は機械学習を用いて自然言語処理をする方法について触れています。例えば、文章がポジティブかネガティブか判断する文章分類の処理を行いたいとします。"bad"という単語が含まれるとネガティブ、"good"という単語が含まれるとポジティブというルールベースでの分類では限界があります。自然言語による入力をあるパラメータによって変換して出力することでその処置が行えると仮定し、適切なパラメータを探し出すために正しい入力-出力のペアで機械学習を行うことで、ルールベースよりも汎用な処理が高速に作成できます。パラメータを多層にしたニューラルネットワークにより複雑なタスクができます。

3章は単語ベクトル表現についてです。機械学習では単語をベクトルに変換して処理するので、より単語の性質を表したベクトルに変換する必要があります。単語自体の特徴(例: 品詞、語幹など)を反映する基本的な手法から発展して、文脈によって単語の意味は決まっているという仮説「分布仮説」に基づいてある単語と文脈との関連の強さを計算するアイディアが生まれました。実際に、ニューラルネットワークを使うことで、文脈の中の単語の意味をニューラルネットワークが利用する実数空間に埋め込む「単語埋め込み(word embedding)」が実現しました。

4章は文や文章などの単語より広い範囲についてベクトル表現を扱っています。文全体のベクトルを構成している単語の合成によってを求めるアプローチがいくつかあります。「再帰型ニューラルネットワーク」は、文の最初の単語から順番にベクトルの合成を繰り返すことで、全部の単語の情報が入り、文脈や並びの順番も反映されるメリットがあります。しかし、全ての単語を"記憶"していることが原因で、最初の方の単語の影響が大きすぎてしまい後の単語の影響が小さくなる「勾配消失」や逆に直近の単語の影響が大きすぎて不安定になる「勾配爆発」の問題があります。これらの解決のため、特定のタイミングで"忘却"させる「LSTM」「ゲート付き再帰ユニット」などのアプローチがあります。

5章はベクトル表現を拡張して文章を生成する方法についてです。単語の羅列について、それが文章であるかの確率(自然さ)をモデル化した「言語モデル」というものがあり、与えられた文章が自然な文章かどうかを判別することができます。言語モデルを利用して、前方の文脈から最も自然な次の単語を探索することで文章生成を行うことができます。ニューラル言語モデルでは、前方の文脈のベクトルから次の単語ベクトルを予測するニューラルネットワークのパラメータを学習で求めます。

言語モデルの発展として、入力文を出力文に変換するモデル「系列変換モデル」を使うと、機械翻訳などのタスクを行うことができます。系列変換モデルは、入力の単語の列から特徴ベクトルの作成する「エンコーダ」、特徴ベクトルから出力を作成する「デコーダ」、単語の重要度を判断して重みづけをする「注意機構」がエンコーダをデコーダを繋ぐようにできています。

6章は「Transformer」と呼ばれる系列変換モデルの重要なアーキテクチャについてです。一番の特徴は「自己注意機構」があることで、エンコーダ内で単語ベクトルから周辺単語を考慮したベクトルを学習、再計算することで、文脈に応じたベクトル計算を実現しています。自己注意機構は、系列変換モデルの注意機構や再帰型ニューラルネットワークの忘却の仕組みなどの複数の仕組みをシンプルに統合したような役割があります。Transformerを用いたモデルは様々な分野で高い性能を発揮しています。

7章はTransformerによって発展した大規模な事前学習モデルとそれらモデルを調整するファインチューニングについてです。今まではベクトル化とベクトルを利用するタスクのモデルは別々でしたが、事前学習したベクトル化のモデルのパラメータを後段タスクのモデルでも使うアプローチが現れました。大規模事前学習を行ったモデルとして、「GPT」「BERT」「BART, T5」などがあります。GPTはTransformerのデコーダを採用しており、文章生成に強い特徴があります。BERTはエンコーダを採用し、文章分類などに強いです。BART, T5はBERTとGPTを組み合わせており、GPTと比較して、要約、対話、機械翻訳など入力文に強く条件づけられた言語生成タスクに強いです。

ベクトル化モデルのパラメータを後段タスクのモデルでも使う際に、訓練データを用いてパラメータを調整することで、特定のタスクやトピック(例えば特定の言語)が扱えるよう適応させることができ、これを「ファインチューニング」と呼びます。例えば、機械翻訳を行うためにmBART(多言語対応BART)に10万~1000万対訳文章セットの追加学習を行うことで、ゼロから訓練するよりも顕著に高い性能になると報告されています。

パラメータが多い大規模な事前学習モデルは性能がいいですが、動作にマシンスペックや時間が要求されるので、軽くするためのアプローチとして「知識蒸留」があります。大規模モデル(教師モデル)の挙動を真似るように小規模モデル(生徒モデル)を訓練することです。DistilBERTはパラメータ数40%でBERTの97%の性能を発揮していると報告されています。
